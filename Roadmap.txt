Phase 1 ‚Äì Data acquisition & cleaning

Objectif : Construire un dataset propre de prix journaliers pour un petit portefeuille (ex: 5 actions S&P500).

Fichiers :

src/config.py : liste des tickers, chemins, seed, etc.

src/data/load_data.py : t√©l√©chargement via KaggleHub, pivot, filtrage.

src/data/preprocess.py : nettoyage (dates, NaN), calcul des rendements, split temporel train/test.

Actions d√©taill√©es :

T√©l√©charger le dataset S&P500 (5 ans).

S√©lectionner quelques tickers (AAPL, MSFT, AMZN, GOOGL, META).

Transformer en DataFrame prices[date, tickers].

Cr√©er returns = prices.pct_change().

Split train/test (par date, pas par random).

Sorties (pour le rapport) :

Tableau descriptif du dataset : p√©riode, nombre d‚Äôactions, fr√©quence, source (Kaggle).

Graphique simple : prix ou indices normalis√©s de 1 √† la date de d√©part.

Phase 2 ‚Äì Exploratory Data Analysis (EDA)

Objectif : Comprendre le comportement du march√© et justifier l‚Äôint√©r√™t du RL.

Support :

Notebook notebooks/EDA.ipynb (les profs aiment voir √ßa).

Analyses :

Graphiques des prix normalis√©s par action.

Distribution des rendements journaliers (histogrammes).

Heatmap de corr√©lation entre les rendements des actions.

Volatilit√© rolling (ex: 20 jours).

Sorties :

reports/figures/prices_overview.png

reports/figures/returns_distribution.png

reports/figures/correlation_heatmap.png

tu les r√©utilises dans la partie ‚ÄúData exploration‚Äù du rapport.

Phase 3 ‚Äì Feature engineering (states pour le RL)

Objectif : D√©finir ce que l‚Äôagent RL ‚Äúvoit‚Äù (les √©tats).

Fichiers :

src/features/technical_indicators.py

scripts/run_prepare_features.py

Features possibles :

Rendements normalis√©s (z-score) sur une fen√™tre.

Moyennes mobiles : MA20, MA60.

Volatilit√© rolling (20 jours).

Ratio price / MA20.

Optionnel : momentum sur 10 jours.

Actions :

√Ä partir de prices et returns, calculer ces indicateurs.

Construire une matrice d‚Äôobservation sur une fen√™tre temporelle (ex : 20 jours).

Sauvegarder dans data/processed/ (par ex. features_train.pkl, features_test.pkl).

Sorties :

Quelques graphiques illustrant MA et volatilit√© pour 1 action (pour le rapport).

Phase 4 ‚Äì Baseline strategies

Objectif : Avoir une ou deux strat√©gies simples pour comparaison.

Fichiers :

src/baselines/equal_weight.py

src/evaluation/metrics.py

scripts/run_baseline.py (d√©j√† fonctionnel üëç)

Baseline principale :

Equal-Weight Buy & Hold : 1/N sur tous les actifs, sans rebalancing quotidien.

M√©triques :

Cumulative return

Volatility (std des rendements journaliers)

Sharpe ratio (rf = 0)

Max drawdown (√† ajouter)

Sorties :

baseline_equal_weight.png (tu l‚Äôas d√©j√†)

metrics_train_baseline.csv

metrics_test_baseline.csv

Dans le rapport, tu expliques pourquoi cette baseline est raisonnable et pourquoi tu veux la d√©passer.

Phase 5 ‚Äì RL Environment (Gym)

Objectif : Cr√©er un environnement compatible Gym pour pouvoir utiliser PPO.

Fichier :

src/env/portfolio_env.py

Structure :

import gymnasium as gym
from gymnasium import spaces

class PortfolioEnv(gym.Env):
    def __init__(...):
        # observation_space : Box
        # action_space : Box (poids entre 0 et 1 par actif)
    def reset(self, seed=None, options=None):
        ...
    def step(self, action):
        ...


Points importants √† d√©finir :

Observation : fen√™tre des features (par ex. [window, n_assets, n_features] aplatie).

Action : vecteur de poids (somme 1, clamp entre 0 et 1).

Reward : rendement du portefeuille ce jour-l√† (ou log-return).

Episode : une p√©riode compl√®te (train) de ton dataset.

Sorties :

Code propre + commentaires expliquant le choix de reward & state (tr√®s important pour la note ‚Äúcode‚Äù).

Tu pourras ajouter un sch√©ma ‚Äúagent‚Äìenvironnement‚Äù dans le rapport.

Phase 6 ‚Äì Training PPO

Objectif : Entra√Æner un agent PPO √† g√©rer le portefeuille.

Fichier :

src/agents/train_ppo.py

T√¢ches :

Cr√©er l‚Äôenvironnement train : env = PortfolioEnv(prices_train, features_train, ...).

Utiliser Stable-Baselines3 :

from stable_baselines3 import PPO

model = PPO("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=100_000)
model.save("models/ppo_portfolio")


Ajuster √©ventuellement quelques hyperparam√®tres (learning rate, gamma, etc.).

Sorties :

Mod√®le sauvegard√© : models/ppo_portfolio.zip.

Courbe de reward moyen par √©pisode (tu peux la sauvegarder dans reports/figures/ppo_training_reward.png).

Dans le rapport, tu expliques bri√®vement PPO (avec une r√©f√©rence scientifique) et justifies tes choix de param√®tres.

Phase 7 ‚Äì √âvaluation RL vs Baseline

Objectif : Comparer proprement la performance en out-of-sample (test).

Fichier :

src/agents/evaluate_ppo.py

T√¢ches :

Charger prices_test et les features test.

Cr√©er un env test sans exploration.

Charger le mod√®le : model = PPO.load("models/ppo_portfolio").

Boucle :

obs, _ = env.reset()
done = False
equity_curve = []

while not done:
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, done, truncated, info = env.step(action)
    equity_curve.append(info["portfolio_value"])


Construire la courbe d‚Äô√©quit√© RL sur la p√©riode test.

Calculer les m√™mes m√©triques que pour la baseline (Sharpe, volatilit√©, etc.).

Sorties :

reports/figures/equity_rl_vs_baseline_test.png

reports/tables/metrics_test_rl_vs_baseline.csv

C‚Äôest le c≈ìur des r√©sultats pour ton rapport et ta vid√©o.

Phase 8 ‚Äì Robustness & Discussion

Objectif : Montrer que tu as un esprit critique (gros plus pour la note).

Id√©es simples (tu n‚Äôes pas oblig√© de tout faire) :

Tester l‚Äôagent sur un sous-ensemble diff√©rent de tickers.

Changer la p√©riode de test (si possible).

√âtudier la sensibilit√© au co√ªt de transaction (m√™me un 0.1% par trade).

Montrer un cas o√π PPO fait pire que la baseline ‚Üí discuter pourquoi.

3. Plan d√©taill√© du rapport (en anglais)

Le rapport doit contenir (d‚Äôapr√®s les consignes) : business case, dataset, exploration, mod√®les, obstacles, comparaison, conclusion, r√©f√©rences.

Voici un plan que tu peux suivre quasiment tel quel :

Introduction

Context: portfolio management & financial markets

Problem statement: can RL improve portfolio allocation vs a simple strategy?

Contributions of this project

Business Case and Objectives

Description of the portfolio allocation problem

Relevance to your specialization (finance / data science / etc.)

Formalization of the task as a sequential decision problem (states, actions, rewards)

Data Description

Source: Kaggle S&P500 (5 years)

Selected assets and time period

Preprocessing: cleaning, resampling, handling missing values

Train / test split description

Exploratory Data Analysis

Price evolution plots

Return distributions

Correlation between assets

Volatility over time

Short interpretation: diversification, risk, trends

Methodology: Baseline Strategies

Definition of the Equal-Weight Buy & Hold strategy

Implementation details

Evaluation metrics (Sharpe, volatility, drawdown, etc.)

Baseline performance on train/test (tables + figures)

Methodology: Reinforcement Learning Approach

Short introduction to RL and PPO (with 1‚Äì2 references)

Description of the PortfolioEnv:

State / observation space

Action space

Reward definition

Episode structure

Implementation details (libraries, hyperparameters)

Experimental Setup

Training procedure (number of timesteps, hardware, random seeds)

Hyperparameter choices and tuning attempts

Obstacles encountered (instability, overfitting, reward shape, etc.)

Results and Comparison

Equity curves of PPO vs baseline on test set

Table of metrics: cumulative return, Sharpe, volatility, drawdown

Discussion:

Does PPO really outperform the baseline?

In which market regimes?

Trading frequency, turnover, transaction costs

Limitations and Future Work

Limited number of assets

Ignored transaction costs / slippage (or simplified assumptions)

RL instability and reproducibility issues

Possible extensions: more assets, alternative RL algorithms, risk-aware rewards, etc.

Conclusion

Summary of findings

Answer to the initial research question

Practical takeaways

References

Scientific papers on RL in finance and PPO

Dataset link

Libraries / documentation

4. Plan d√©taill√© de la vid√©o (4‚Äì5 min)

Les consignes demandent : 4‚Äì5 minutes, business case, mod√®les, obstacles, r√©sultats, conclusion.

Tu peux suivre ce script (temps approximatif) :

(0:00‚Äì0:40) Introduction & Business Case

‚ÄúIn this project, we address portfolio allocation using Reinforcement Learning.‚Äù

Contexte march√©, S&P500, pourquoi c‚Äôest dur.

(0:40‚Äì1:10) Dataset

Source Kaggle, p√©riode, nombre d‚Äôactions

1‚Äì2 graphes cl√©s (√©volution des prix, corr√©lation)

(1:10‚Äì2:00) Baseline and RL Methodology

Baseline Equal-Weight Buy & Hold

Description rapide de l‚Äôenvironnement RL (state, action, reward)

Quelques mots sur PPO

(2:00‚Äì3:00) Results

Montrer les courbes d‚Äô√©quit√© sur la p√©riode test

Montrer la table de m√©triques

Dire clairement : ‚ÄúPPO outperforms / underperforms the baseline‚Äù

(3:00‚Äì3:40) Obstacles & Lessons Learned

Probl√®mes rencontr√©s (instabilit√©, tuning, features‚Ä¶)

Comment vous les avez r√©solus ou contourn√©s

(3:40‚Äì4:10) Conclusion

Rappel de la question de d√©part

R√©ponse simple + limitations + pistes futures

(Optionnel jusqu‚Äô√† 5:00)

Bonus : discuter bri√®vement des risques d‚Äôutiliser RL en finance r√©elle.